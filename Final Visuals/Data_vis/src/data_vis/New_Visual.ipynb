{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised Machine Learning Attempt:\n",
    "    -Predicting the Winner provided the data \n",
    "    -Looking to learn from the data in the CSV file to predict the winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(r'C:\\Users\\Cdog1\\OneDrive\\Documents\\MEM 680T\\Final Visuals\\Data_vis\\src\\data_vis\\datapath\\500069182.csv')  # Replace 'your_data.csv' with your actual file path\n",
    "\n",
    "# Select relevant columns\n",
    "columns_to_select = ['winner.card1.id', 'winner.card2.id', 'winner.card3.id', 'winner.card4.id', 'winner.card5.id',\n",
    "                      'winner.card6.id', 'winner.card7.id', 'winner.card8.id', 'loser.card1.id', 'loser.card2.id',\n",
    "                      'loser.card3.id', 'loser.card4.id', 'loser.card5.id', 'loser.card6.id', 'loser.card7.id',\n",
    "                      'loser.card8.id', 'winner.startingTrophies', 'loser.startingTrophies', 'winner.crowns', 'loser.crowns']\n",
    "\n",
    "df_selected = df[columns_to_select]\n",
    "\n",
    "# Handle missing values\n",
    "df_selected = df_selected.dropna()\n",
    "\n",
    "# Separate features and labels\n",
    "X = df_selected.drop(['winner.crowns', 'loser.crowns'], axis=1)\n",
    "y = df_selected[['winner.crowns', 'loser.crowns']]\n",
    "\n",
    "# Standard scaling for numerical features\n",
    "numeric_features = ['winner.startingTrophies', 'loser.startingTrophies']\n",
    "scaler = StandardScaler()\n",
    "X[numeric_features] = scaler.fit_transform(X[numeric_features])\n",
    "\n",
    "# Neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(2, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mse', optimizer=Adam(lr=0.001), metrics=['mae'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# Make predictions on new data (100 different games)\n",
    "new_data = pd.DataFrame({\n",
    "    'winner.card1.id': [random.choice(df['winner.card1.id'].dropna()) for _ in range(100)],\n",
    "    'loser.card1.id': [random.choice(df['loser.card1.id'].dropna()) for _ in range(100)],\n",
    "    'winner.card2.id': [random.choice(df['winner.card2.id'].dropna()) for _ in range(100)],\n",
    "    'loser.card2.id': [random.choice(df['loser.card2.id'].dropna()) for _ in range(100)],\n",
    "    'winner.card3.id': [random.choice(df['winner.card3.id'].dropna()) for _ in range(100)],\n",
    "    'loser.card3.id': [random.choice(df['loser.card3.id'].dropna()) for _ in range(100)],\n",
    "    'winner.card4.id': [random.choice(df['winner.card4.id'].dropna()) for _ in range(100)],\n",
    "    'loser.card4.id': [random.choice(df['loser.card4.id'].dropna()) for _ in range(100)],\n",
    "    'winner.card5.id': [random.choice(df['winner.card5.id'].dropna()) for _ in range(100)],\n",
    "    'loser.card5.id': [random.choice(df['loser.card5.id'].dropna()) for _ in range(100)],\n",
    "    'winner.card6.id': [random.choice(df['winner.card6.id'].dropna()) for _ in range(100)],\n",
    "    'loser.card6.id': [random.choice(df['loser.card6.id'].dropna()) for _ in range(100)],\n",
    "    'winner.card7.id': [random.choice(df['winner.card7.id'].dropna()) for _ in range(100)],\n",
    "    'loser.card7.id': [random.choice(df['loser.card7.id'].dropna()) for _ in range(100)],\n",
    "    'winner.card8.id': [random.choice(df['winner.card8.id'].dropna()) for _ in range(100)],\n",
    "    'loser.card8.id': [random.choice(df['loser.card8.id'].dropna()) for _ in range(100)],\n",
    "    'winner.startingTrophies': np.random.randint(0, 5000, size=100),\n",
    "    'loser.startingTrophies': np.random.randint(0, 5000, size=100)\n",
    "})\n",
    "\n",
    "predictions = model.predict(new_data)\n",
    "rounded_predictions = predictions.astype(int)\n",
    "print(\"Generated new data:\")\n",
    "print(new_data)\n",
    "print(\"\\nPredicted Winner Crowns:\")\n",
    "print(predictions[:, 0])\n",
    "print(\"\\nPredicted Loser Crowns:\")\n",
    "print(np.clip(predictions[:, 1], 0, predictions[:, 0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the outcomes\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Bar plot for predicted winner crowns\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.bar(range(len(rounded_predictions[:, 0])), rounded_predictions[:, 0], color='blue')\n",
    "plt.title('Predicted Winner Crowns')\n",
    "plt.xlabel('Game Number')\n",
    "plt.ylabel('Crowns')\n",
    "\n",
    "# Bar plot for predicted loser crowns\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.bar(range(len(rounded_predictions[:, 1])), rounded_predictions[:, 1], color='red')\n",
    "plt.title('Predicted Loser Crowns')\n",
    "plt.xlabel('Game Number')\n",
    "plt.ylabel('Crowns')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter plot for predicted winner vs. loser crowns\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(rounded_predictions[:, 0], rounded_predictions[:, 1], c='blue', label='Predicted Crowns')\n",
    "plt.title('Predicted Winner vs. Loser Crowns')\n",
    "plt.xlabel('Predicted Winner Crowns')\n",
    "plt.ylabel('Predicted Loser Crowns')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Cdog1\\anaconda3\\envs\\Final\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Cdog1\\anaconda3\\envs\\Final\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\Cdog1\\anaconda3\\envs\\Final\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Cdog1\\anaconda3\\envs\\Final\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10486/10486 - 8s - loss: 93193500622848.0000 - val_loss: 8837237047296.0000 - 8s/epoch - 722us/step\n",
      "Epoch 2/10\n",
      "10486/10486 - 7s - loss: 79378746703872.0000 - val_loss: 5439264653312.0000 - 7s/epoch - 698us/step\n",
      "Epoch 3/10\n",
      "10486/10486 - 7s - loss: 79430856736768.0000 - val_loss: 11658522001408.0000 - 7s/epoch - 682us/step\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Cdog1\\OneDrive\\Documents\\MEM 680T\\Final Visuals\\Data_vis\\src\\data_vis\\New_Visual.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cdog1/OneDrive/Documents/MEM%20680T/Final%20Visuals/Data_vis/src/data_vis/New_Visual.ipynb#X11sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m X_train, X_test \u001b[39m=\u001b[39m train_test_split(X, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cdog1/OneDrive/Documents/MEM%20680T/Final%20Visuals/Data_vis/src/data_vis/New_Visual.ipynb#X11sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# Train the autoencoder\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Cdog1/OneDrive/Documents/MEM%20680T/Final%20Visuals/Data_vis/src/data_vis/New_Visual.ipynb#X11sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m autoencoder\u001b[39m.\u001b[39;49mfit(X_train, X_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cdog1/OneDrive/Documents/MEM%20680T/Final%20Visuals/Data_vis/src/data_vis/New_Visual.ipynb#X11sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# Encode the data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cdog1/OneDrive/Documents/MEM%20680T/Final%20Visuals/Data_vis/src/data_vis/New_Visual.ipynb#X11sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m encoded_X_train \u001b[39m=\u001b[39m autoencoder\u001b[39m.\u001b[39mpredict(X_train)\n",
      "File \u001b[1;32mc:\\Users\\Cdog1\\anaconda3\\envs\\Final\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Cdog1\\anaconda3\\envs\\Final\\lib\\site-packages\\keras\\src\\engine\\training.py:1798\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1796\u001b[0m callbacks\u001b[39m.\u001b[39mon_epoch_begin(epoch)\n\u001b[0;32m   1797\u001b[0m \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m-> 1798\u001b[0m     \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[0;32m   1799\u001b[0m         \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m             epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m             _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m         ):\n\u001b[0;32m   1806\u001b[0m             callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n",
      "File \u001b[1;32mc:\\Users\\Cdog1\\anaconda3\\envs\\Final\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1411\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1409\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1411\u001b[0m original_spe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m   1412\u001b[0m can_run_full_execution \u001b[39m=\u001b[39m (\n\u001b[0;32m   1413\u001b[0m     original_spe \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1414\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1415\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m original_spe\n\u001b[0;32m   1416\u001b[0m )\n\u001b[0;32m   1418\u001b[0m \u001b[39mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[1;32mc:\\Users\\Cdog1\\anaconda3\\envs\\Final\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:689\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnumpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    688\u001b[0m   \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 689\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_value()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    690\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    691\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Cdog1\\anaconda3\\envs\\Final\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:839\u001b[0m, in \u001b[0;36mBaseResourceVariable.read_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Constructs an op which reads the value of this variable.\u001b[39;00m\n\u001b[0;32m    831\u001b[0m \n\u001b[0;32m    832\u001b[0m \u001b[39mShould be used when there are multiple reads, or when it is desirable to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[39m  The value of the variable.\u001b[39;00m\n\u001b[0;32m    837\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    838\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(\u001b[39m\"\u001b[39m\u001b[39mRead\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 839\u001b[0m   value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_variable_op()\n\u001b[0;32m    840\u001b[0m \u001b[39m# Return an identity so it can get placed on whatever device the context\u001b[39;00m\n\u001b[0;32m    841\u001b[0m \u001b[39m# specifies instead of the device where the variable is.\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \u001b[39mreturn\u001b[39;00m array_ops\u001b[39m.\u001b[39midentity(value)\n",
      "File \u001b[1;32mc:\\Users\\Cdog1\\anaconda3\\envs\\Final\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:818\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op\u001b[1;34m(self, no_copy)\u001b[0m\n\u001b[0;32m    816\u001b[0m       result \u001b[39m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    817\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 818\u001b[0m   result \u001b[39m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    820\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    821\u001b[0m   \u001b[39m# Note that if a control flow context is active the input of the read op\u001b[39;00m\n\u001b[0;32m    822\u001b[0m   \u001b[39m# might not actually be the handle. This line bypasses it.\u001b[39;00m\n\u001b[0;32m    823\u001b[0m   record\u001b[39m.\u001b[39mrecord_operation(\n\u001b[0;32m    824\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mReadVariableOp\u001b[39m\u001b[39m\"\u001b[39m, [result], [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle],\n\u001b[0;32m    825\u001b[0m       backward_function\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: [x],\n\u001b[0;32m    826\u001b[0m       forward_function\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: [x])\n",
      "File \u001b[1;32mc:\\Users\\Cdog1\\anaconda3\\envs\\Final\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:808\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001b[1;34m(no_copy)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[39mif\u001b[39;00m no_copy \u001b[39mand\u001b[39;00m forward_compat\u001b[39m.\u001b[39mforward_compatible(\u001b[39m2022\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m3\u001b[39m):\n\u001b[0;32m    807\u001b[0m   gen_resource_variable_ops\u001b[39m.\u001b[39mdisable_copy_on_read(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle)\n\u001b[1;32m--> 808\u001b[0m result \u001b[39m=\u001b[39m gen_resource_variable_ops\u001b[39m.\u001b[39;49mread_variable_op(\n\u001b[0;32m    809\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dtype)\n\u001b[0;32m    810\u001b[0m _maybe_set_handle_data(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dtype, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle, result)\n\u001b[0;32m    811\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Cdog1\\anaconda3\\envs\\Final\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:535\u001b[0m, in \u001b[0;36mread_variable_op\u001b[1;34m(resource, dtype, name)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m    534\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 535\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m    536\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mReadVariableOp\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, resource, \u001b[39m\"\u001b[39;49m\u001b[39mdtype\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype)\n\u001b[0;32m    537\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m    538\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(r'C:\\Users\\Cdog1\\OneDrive\\Documents\\MEM 680T\\Final Visuals\\Data_vis\\src\\data_vis\\datapath\\500069182.csv')\n",
    "\n",
    "# Select relevant columns\n",
    "columns_to_select = ['winner.card1.id', 'winner.card2.id', 'winner.card3.id', 'winner.card4.id', 'winner.card5.id',\n",
    "                      'winner.card6.id', 'winner.card7.id', 'winner.card8.id', 'loser.card1.id', 'loser.card2.id',\n",
    "                      'loser.card3.id', 'loser.card4.id', 'loser.card5.id', 'loser.card6.id', 'loser.card7.id',\n",
    "                      'loser.card8.id', 'winner.startingTrophies', 'loser.startingTrophies', 'winner.crowns', 'loser.crowns']\n",
    "\n",
    "df_selected = df[columns_to_select]\n",
    "\n",
    "# Handle missing values\n",
    "df_selected = df_selected.dropna()\n",
    "\n",
    "# Separate features\n",
    "X = df_selected.drop(['winner.crowns', 'loser.crowns'], axis=1)\n",
    "\n",
    "# Standard scaling for numerical features\n",
    "numeric_features = ['winner.startingTrophies', 'loser.startingTrophies']\n",
    "scaler = StandardScaler()\n",
    "X[numeric_features] = scaler.fit_transform(X[numeric_features])\n",
    "\n",
    "# Neural network model (Autoencoder)\n",
    "input_dim = X.shape[1]\n",
    "encoding_dim = 20  # Adjust the encoding dimension as needed\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoder = Dense(encoding_dim, activation='relu', activity_regularizer=regularizers.l1(1e-4))(input_layer)\n",
    "encoder = Dropout(0.5)(encoder)\n",
    "\n",
    "decoder = Dense(input_dim, activation='linear')(encoder)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "autoencoder.compile(optimizer=Adam(lr=0.0001), loss='mse') \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(X_train, X_train, epochs=10, batch_size=64, validation_split=0.2, verbose=2)\n",
    "\n",
    "# Encode the data\n",
    "encoded_X_train = autoencoder.predict(X_train)\n",
    "encoded_X_test = autoencoder.predict(X_test)\n",
    "\n",
    "# Display the encoded representations\n",
    "print(\"Encoded representations for X_train:\")\n",
    "print(encoded_X_train)\n",
    "\n",
    "print(\"\\nEncoded representations for X_test:\")\n",
    "print(encoded_X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder.fit(X_train, X_train, epochs=20, batch_size=64, validation_split=0.2, verbose=2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
